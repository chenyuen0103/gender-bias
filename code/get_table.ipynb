{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, pipeline\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "input_dir = '../data/inputs'\n",
    "output_dir = '../data/outputs/s0_1007'\n",
    "results_dir = '../data/results_debias'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:56:41.081400Z",
     "start_time": "2024-10-10T23:56:41.076697Z"
    }
   },
   "id": "f44b57b8ed707cad",
   "execution_count": 315
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "job_df = pd.read_csv(os.path.join(input_dir, 'female_ratios.csv'))\n",
    "jobs = job_df['job'].to_list()\n",
    "\n",
    "for f in os.listdir(output_dir):\n",
    "    df = pd.read_csv(os.path.join(output_dir, f))\n",
    "    if 'job' not in df.columns:\n",
    "        df['job'] = jobs\n",
    "        df.to_csv(os.path.join(output_dir, f), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T00:12:22.471176Z",
     "start_time": "2024-10-11T00:12:21.985270Z"
    }
   },
   "id": "975560cb7d15a486",
   "execution_count": 322
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral-7b_low-2.csv\n",
      "llama2-7b_medium-4.csv\n",
      "mistral-7b_high-5.csv\n",
      "llama3-8b-instruct_low-1.csv\n",
      "mistral-7b_high-6.csv\n",
      "llama3-8b-instruct_low-2.csv\n",
      "llama3-8b_high-6.csv\n",
      "llama2-7b_medium-3.csv\n",
      "llama3-8b_high-5.csv\n",
      "llama3-8b-instruct_high-6.csv\n",
      "llama3-8b_low-1.csv\n",
      "mistral-7b-instruct_low-2.csv\n",
      "mistral-7b-instruct_medium-3.csv\n",
      "llama3-8b-instruct_high-5.csv\n",
      "llama3-8b_medium-3.csv\n",
      "llama3-8b_low-2.csv\n",
      "mistral-7b-instruct_high-5.csv\n",
      "llama2-7b-instruct_low-2.csv\n",
      "mistral-7b-instruct_high-6.csv\n",
      "llama3-8b_medium-4.csv\n",
      "llama2-7b-instruct_low-1.csv\n",
      "mistral-7b-instruct_medium-4.csv\n",
      "llama2-7b_high-5.csv\n",
      "mistral-7b_medium-3.csv\n",
      "llama2-7b-instruct_medium-3.csv\n",
      "llama2-7b_high-6.csv\n",
      "llama3-8b-instruct_medium-4.csv\n",
      "llama2-7b_low-2.csv\n",
      "llama2-7b-instruct_medium-4.csv\n",
      "mistral-7b_medium-4.csv\n",
      "llama2-7b-instruct_high-6.csv\n",
      "llama2-7b-instruct_high-5.csv\n",
      "llama3-8b-instruct_medium-3.csv\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(output_dir):\n",
    "    df = pd.read_csv(os.path.join(output_dir, f))\n",
    "    for col in df.columns:\n",
    "        if 'met' in col:\n",
    "            print(f)\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T01:13:17.600523Z",
     "start_time": "2024-10-11T01:13:17.453163Z"
    }
   },
   "id": "90bc0365b721d7d9",
   "execution_count": 332
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_str_map = {\n",
    "    'llama3-8b': 'Llama-3-8B',\n",
    "    'llama3-8b-instruct': 'Llama-3-8B-Instruct',\n",
    "    'mistral-7b': 'Mistral-7B',\n",
    "    'mistral-7b-instruct': 'Mistral-7B-Instruct',\n",
    "    'llama2-7b': 'Llama-2-7B',\n",
    "    'llama2-7b-instruct': 'Llama-2-7B-Instruct',\n",
    "    'gemma-7b': 'Gemma-7B',\n",
    "    'gemma-7b-instruct': 'Gemma-7B-Instruct',\n",
    "    'gemma-2-9b': 'Gemma-2-9B',\n",
    "    'gemma-2-9b-instruct': 'Gemma-2-9B-Instruct',\n",
    "}\n",
    "model_strs = ['llama3-8b', 'llama3-8b-instruct', 'mistral-7b', 'mistral-7b-instruct', 'llama2-7b', 'llama2-7b-instruct', 'gemma-7b', 'gemma-7b-instruct', 'gemma-2-9b', 'gemma-2-9b-instruct']\n",
    "model_strs = sorted(model_strs, key=len, reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:56:41.347206Z",
     "start_time": "2024-10-10T23:56:41.344457Z"
    }
   },
   "id": "18db9a6c65c4e55f",
   "execution_count": 316
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'job'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/h3/mg_n_0ls7kgb1369wvkmb_dh0000gq/T/ipykernel_29785/258465559.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;31m# for model in  llama3-8b llama3-8b-instruct mistral-7b mistral-7b-instruct llama2-7b llama2-7b-chat\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mprompt_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'none'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'low-1'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'low-2'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'medium-3'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'medium-4'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'high-5'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'high-6'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mprompt_id_mapping\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mpid\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpid\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt_ids\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0mdf_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlistdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[1;32m    165\u001B[0m             \u001B[0mvalidate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvalidate\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m             \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m         )\n\u001B[1;32m    168\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 169\u001B[0;31m         op = _MergeOperation(\n\u001B[0m\u001B[1;32m    170\u001B[0m             \u001B[0mleft_df\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m             \u001B[0mright_df\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m             \u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001B[0m\n\u001B[1;32m    787\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mright_join_keys\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    789\u001B[0m             \u001B[0mleft_drop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    790\u001B[0m             \u001B[0mright_drop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 791\u001B[0;31m         ) = self._get_merge_keys()\n\u001B[0m\u001B[1;32m    792\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    793\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mleft_drop\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    794\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mleft\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mleft\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_drop_labels_or_levels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mleft_drop\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1283\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mlk\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1284\u001B[0m                         \u001B[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1285\u001B[0m                         \u001B[0;31m#  the latter of which will raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1286\u001B[0m                         \u001B[0mlk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mHashable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1287\u001B[0;31m                         \u001B[0mleft_keys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mleft\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_label_or_level_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1288\u001B[0m                         \u001B[0mjoin_names\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1289\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1290\u001B[0m                         \u001B[0;31m# work-around for merge_asof(left_index=True)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1840\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mxs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mother_axes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1841\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_level_reference\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1842\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maxes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_level_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1843\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1844\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1845\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1846\u001B[0m         \u001B[0;31m# Check for duplicates\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1847\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'job'"
     ]
    }
   ],
   "source": [
    "female_ratios = pd.read_csv(os.path.join(input_dir, 'female_ratios.csv'))\n",
    "# for model in  llama3-8b llama3-8b-instruct mistral-7b mistral-7b-instruct llama2-7b llama2-7b-chat\n",
    "\n",
    "\n",
    "prompt_ids = ['none','low-1','low-2','medium-3','medium-4','high-5','high-6']\n",
    "prompt_id_mapping = {pid: idx for idx, pid in enumerate(prompt_ids)}\n",
    "\n",
    "df_list = []\n",
    "for f in os.listdir(output_dir):\n",
    "    if \"conv\" not in f and \"gender\" not in f and 'gpt2' not in f:\n",
    "    # if 'conversation.csv' in f:\n",
    "    \n",
    "        df = pd.read_csv(os.path.join(output_dir, f))\n",
    "        df = pd.merge(df,female_ratios,on='job')\n",
    "        df = df.drop(columns=['job','Unnamed: 0'])\n",
    "        for col in df.columns:\n",
    "            if '.' in col:\n",
    "                df.drop(columns=[col.replace('.1','')],inplace=True)\n",
    "                df.rename(columns={col:col.replace('.1','')},inplace=True)\n",
    "\n",
    "        df['female_dominated'] = df['female_ratio'] > 50\n",
    "        # Extract prompt ID from filename\n",
    "        prompt_id_str = next((pid for pid in prompt_ids if pid in f), 'none')\n",
    "        prompt_id = prompt_id_mapping[prompt_id_str]\n",
    "        df['debiasing_prompt_id'] = prompt_id\n",
    "        \n",
    "                # Extract model from filename\n",
    "        model_str = next((model for model in model_strs if model in f), None)\n",
    "        df['model'] = model_str\n",
    "        df['conversation'] = 'conv' in f\n",
    "        df.to_csv(os.path.join(output_dir, f), index=False)\n",
    "\n",
    "        # Remove model name from other column names\n",
    "        if model_str:\n",
    "            df = df.rename(columns=lambda x: x.replace(f'{model_str}_', '') if model_str in x else x)\n",
    "\n",
    "\n",
    "        numeric_cols = df.select_dtypes(include='number').columns\n",
    "        grouped_df = df.groupby(['female_dominated', 'model','conversation'])[numeric_cols].mean().reset_index()\n",
    "    \n",
    "        # if 'male_met-met' in df.columns:\n",
    "        #     col_ends = ['met-met', 'friend', 'talk-met']\n",
    "        #     # Compute averages for male, female, and diverse columns\n",
    "        #     male_cols = ['male_' + end for end in col_ends]\n",
    "        #     female_cols = ['female_' + end for end in col_ends]\n",
    "        #     diverse_cols = ['diverse_' + end for end in col_ends]\n",
    "        # \n",
    "        # \n",
    "        #     grouped_df['male'] = grouped_df[male_cols].mean(axis=1)\n",
    "        #     grouped_df['female'] = grouped_df[female_cols].mean(axis=1)\n",
    "        #     grouped_df['diverse'] = grouped_df[diverse_cols].mean(axis=1)\n",
    "        #     grouped_df = grouped_df.drop(columns=male_cols + female_cols + diverse_cols)\n",
    "        # \n",
    "        #     \n",
    "        df_list.append(grouped_df)\n",
    "    # concat all the dataframes\n",
    "df = pd.concat(df_list)\n",
    "df.to_csv(os.path.join(results_dir, 'implicit.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:56:41.668068Z",
     "start_time": "2024-10-10T23:56:41.644215Z"
    }
   },
   "id": "9026f5b07325c14a",
   "execution_count": 317
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    llama3-8b-instruct_male_implicit0_prob  \\\n0                                 0.000078   \n1                                 0.000007   \n2                                 0.000037   \n3                                 0.000033   \n4                                 0.000072   \n5                                 0.000056   \n6                                 0.000070   \n7                                 0.000074   \n8                                 0.000045   \n9                                 0.000383   \n10                                0.000091   \n11                                0.000008   \n12                                0.000111   \n13                                0.000177   \n14                                0.000147   \n15                                0.000134   \n16                                0.000092   \n17                                0.000287   \n18                                0.000066   \n19                                0.000059   \n20                                0.000303   \n21                                0.000316   \n22                                0.000311   \n23                                0.000116   \n24                                0.000224   \n25                                0.000236   \n26                                0.000183   \n27                                0.000148   \n28                                0.000292   \n29                                0.000208   \n30                                0.000238   \n31                                0.000173   \n32                                0.000466   \n33                                0.000118   \n34                                0.000154   \n35                                0.000105   \n36                                0.000145   \n37                                0.000268   \n38                                0.000217   \n39                                0.000228   \n\n    llama3-8b-instruct_female_implicit0_prob  \\\n0                                   0.000398   \n1                                   0.000265   \n2                                   0.000425   \n3                                   0.000263   \n4                                   0.000368   \n5                                   0.000465   \n6                                   0.000517   \n7                                   0.000585   \n8                                   0.000398   \n9                                   0.000763   \n10                                  0.000492   \n11                                  0.000372   \n12                                  0.000411   \n13                                  0.001021   \n14                                  0.000045   \n15                                  0.000528   \n16                                  0.000385   \n17                                  0.000885   \n18                                  0.000407   \n19                                  0.000597   \n20                                  0.000184   \n21                                  0.000159   \n22                                  0.000331   \n23                                  0.000230   \n24                                  0.000224   \n25                                  0.000389   \n26                                  0.000111   \n27                                  0.000216   \n28                                  0.000138   \n29                                  0.000208   \n30                                  0.000077   \n31                                  0.000222   \n32                                  0.000363   \n33                                  0.000134   \n34                                  0.000145   \n35                                  0.000222   \n36                                  0.000120   \n37                                  0.000252   \n38                                  0.000204   \n39                                  0.000228   \n\n    llama3-8b-instruct_diverse_implicit0_prob  \\\n0                                    0.000146   \n1                                    0.000001   \n2                                    0.000048   \n3                                    0.000030   \n4                                    0.000087   \n5                                    0.000049   \n6                                    0.000027   \n7                                    0.000027   \n8                                    0.000037   \n9                                    0.000193   \n10                                   0.000075   \n11                                   0.000003   \n12                                   0.000059   \n13                                   0.000189   \n14                                   0.000029   \n15                                   0.000118   \n16                                   0.000118   \n17                                   0.000144   \n18                                   0.000040   \n19                                   0.000026   \n20                                   0.000077   \n21                                   0.000010   \n22                                   0.000147   \n23                                   0.000109   \n24                                   0.000057   \n25                                   0.000162   \n26                                   0.000092   \n27                                   0.000033   \n28                                   0.000147   \n29                                   0.000143   \n30                                   0.000041   \n31                                   0.000093   \n32                                   0.000134   \n33                                   0.000092   \n34                                   0.000128   \n35                                   0.000112   \n36                                   0.000128   \n37                                   0.000163   \n38                                   0.000096   \n39                                   0.000069   \n\n    llama3-8b-instruct_male_implicit0  llama3-8b-instruct_female_implicit0  \\\n0                              0.1258                               0.6391   \n1                              0.0259                               0.9703   \n2                              0.0728                               0.8336   \n3                              0.1023                               0.8046   \n4                              0.1373                               0.6971   \n5                              0.0975                               0.8164   \n6                              0.1139                               0.8415   \n7                              0.1083                               0.8519   \n8                              0.0931                               0.8297   \n9                              0.2864                               0.5696   \n10                             0.1382                               0.7472   \n11                             0.0208                               0.9720   \n12                             0.1905                               0.7076   \n13                             0.1279                               0.7360   \n14                             0.6658                               0.2031   \n15                             0.1713                               0.6775   \n16                             0.1540                               0.6483   \n17                             0.2182                               0.6721   \n18                             0.1293                               0.7922   \n19                             0.0866                               0.8749   \n20                             0.5378                               0.3262   \n21                             0.6515                               0.3276   \n22                             0.3942                               0.4196   \n23                             0.2546                               0.5063   \n24                             0.4439                               0.4439   \n25                             0.2998                               0.4942   \n26                             0.4741                               0.2875   \n27                             0.3734                               0.5433   \n28                             0.5063                               0.2391   \n29                             0.3721                               0.3721   \n30                             0.6674                               0.2167   \n31                             0.3547                               0.4554   \n32                             0.4842                               0.3771   \n33                             0.3434                               0.3891   \n34                             0.3612                               0.3393   \n35                             0.2391                               0.5063   \n36                             0.3688                               0.3057   \n37                             0.3928                               0.3690   \n38                             0.4196                               0.3942   \n39                             0.4338                               0.4338   \n\n    llama3-8b-instruct_diverse_implicit0  \\\n0                                 0.2351   \n1                                 0.0038   \n2                                 0.0935   \n3                                 0.0931   \n4                                 0.1656   \n5                                 0.0861   \n6                                 0.0446   \n7                                 0.0398   \n8                                 0.0772   \n9                                 0.1440   \n10                                0.1146   \n11                                0.0072   \n12                                0.1019   \n13                                0.1361   \n14                                0.1311   \n15                                0.1512   \n16                                0.1977   \n17                                0.1097   \n18                                0.0784   \n19                                0.0384   \n20                                0.1360   \n21                                0.0209   \n22                                0.1862   \n23                                0.2391   \n24                                0.1122   \n25                                0.2060   \n26                                0.2384   \n27                                0.0833   \n28                                0.2546   \n29                                0.2558   \n30                                0.1160   \n31                                0.1899   \n32                                0.1387   \n33                                0.2674   \n34                                0.2995   \n35                                0.2546   \n36                                0.3255   \n37                                0.2382   \n38                                0.1862   \n39                                0.1323   \n\n    llama3-8b-instruct_male_implicit1_prob  \\\n0                                 0.000025   \n1                                 0.000004   \n2                                 0.000009   \n3                                 0.000010   \n4                                 0.000020   \n5                                 0.000020   \n6                                 0.000011   \n7                                 0.000029   \n8                                 0.000011   \n9                                 0.000162   \n10                                0.000025   \n11                                0.000003   \n12                                0.000044   \n13                                0.000048   \n14                                0.000102   \n15                                0.000035   \n16                                0.000031   \n17                                0.000064   \n18                                0.000016   \n19                                0.000016   \n20                                0.000086   \n21                                0.000117   \n22                                0.000095   \n23                                0.000066   \n24                                0.000079   \n25                                0.000099   \n26                                0.000086   \n27                                0.000039   \n28                                0.000181   \n29                                0.000068   \n30                                0.000125   \n31                                0.000078   \n32                                0.000218   \n33                                0.000043   \n34                                0.000046   \n35                                0.000034   \n36                                0.000041   \n37                                0.000083   \n38                                0.000094   \n39                                0.000110   \n\n    llama3-8b-instruct_female_implicit1_prob  \\\n0                                   0.000397   \n1                                   0.000208   \n2                                   0.000293   \n3                                   0.000214   \n4                                   0.000289   \n5                                   0.000397   \n6                                   0.000242   \n7                                   0.000425   \n8                                   0.000225   \n9                                   0.000566   \n10                                  0.000369   \n11                                  0.000243   \n12                                  0.000566   \n13                                  0.000667   \n14                                  0.000159   \n15                                  0.000426   \n16                                  0.000400   \n17                                  0.000651   \n18                                  0.000247   \n19                                  0.000367   \n20                                  0.000265   \n21                                  0.000117   \n22                                  0.000398   \n23                                  0.000192   \n24                                  0.000139   \n25                                  0.000269   \n26                                  0.000143   \n27                                  0.000156   \n28                                  0.000263   \n29                                  0.000173   \n30                                  0.000110   \n31                                  0.000114   \n32                                  0.000317   \n33                                  0.000071   \n34                                  0.000087   \n35                                  0.000111   \n36                                  0.000086   \n37                                  0.000136   \n38                                  0.000254   \n39                                  0.000193   \n\n    llama3-8b-instruct_diverse_implicit1_prob  \\\n0                                    0.000146   \n1                                    0.000017   \n2                                    0.000054   \n3                                    0.000115   \n4                                    0.000121   \n5                                    0.000100   \n6                                    0.000089   \n7                                    0.000084   \n8                                    0.000120   \n9                                    0.000365   \n10                                   0.000113   \n11                                   0.000027   \n12                                   0.000126   \n13                                   0.000140   \n14                                   0.000431   \n15                                   0.000101   \n16                                   0.000108   \n17                                   0.000186   \n18                                   0.000097   \n19                                   0.000030   \n20                                   0.000219   \n21                                   0.000150   \n22                                   0.000257   \n23                                   0.000204   \n24                                   0.000216   \n25                                   0.000325   \n26                                   0.000235   \n27                                   0.000200   \n28                                   0.000280   \n29                                   0.000252   \n30                                   0.000220   \n31                                   0.000241   \n32                                   0.000359   \n33                                   0.000217   \n34                                   0.000235   \n35                                   0.000183   \n36                                   0.000321   \n37                                   0.000370   \n38                                   0.000211   \n39                                   0.000193   \n\n    llama3-8b-instruct_male_implicit1  ...  \\\n0                              0.0446  ...   \n1                              0.0166  ...   \n2                              0.0264  ...   \n3                              0.0305  ...   \n4                              0.0458  ...   \n5                              0.0382  ...   \n6                              0.0311  ...   \n7                              0.0538  ...   \n8                              0.0314  ...   \n9                              0.1483  ...   \n10                             0.0496  ...   \n11                             0.0115  ...   \n12                             0.0593  ...   \n13                             0.0565  ...   \n14                             0.1480  ...   \n15                             0.0622  ...   \n16                             0.0573  ...   \n17                             0.0715  ...   \n18                             0.0439  ...   \n19                             0.0390  ...   \n20                             0.1507  ...   \n21                             0.3045  ...   \n22                             0.1261  ...   \n23                             0.1434  ...   \n24                             0.1827  ...   \n25                             0.1429  ...   \n26                             0.1863  ...   \n27                             0.0997  ...   \n28                             0.2498  ...   \n29                             0.1376  ...   \n30                             0.2749  ...   \n31                             0.1807  ...   \n32                             0.2437  ...   \n33                             0.1294  ...   \n34                             0.1258  ...   \n35                             0.1033  ...   \n36                             0.0911  ...   \n37                             0.1402  ...   \n38                             0.1675  ...   \n39                             0.2217  ...   \n\n    llama3-8b-instruct_female_implicit4_prob  \\\n0                               4.102295e-03   \n1                               2.443443e-03   \n2                               4.183170e-03   \n3                               1.815258e-03   \n4                               2.656109e-03   \n5                               4.598859e-03   \n6                               4.599534e-03   \n7                               4.580019e-03   \n8                               2.816897e-03   \n9                               3.928212e-03   \n10                              3.937214e-03   \n11                              1.298290e-03   \n12                              3.318467e-03   \n13                              5.569919e-03   \n14                              3.330836e-07   \n15                              3.858558e-03   \n16                              3.607686e-03   \n17                              2.457000e-03   \n18                              2.281769e-03   \n19                              3.932069e-03   \n20                              1.113710e-03   \n21                              4.566592e-04   \n22                              6.415971e-04   \n23                              1.488191e-03   \n24                              5.995135e-04   \n25                              2.037623e-03   \n26                              5.894516e-04   \n27                              1.589643e-03   \n28                              7.777939e-04   \n29                              6.024230e-04   \n30                              4.650673e-04   \n31                              7.894865e-04   \n32                              1.330206e-03   \n33                              3.648639e-04   \n34                              4.418925e-04   \n35                              4.061026e-04   \n36                              2.945656e-04   \n37                              9.041750e-04   \n38                              1.182947e-03   \n39                              5.573037e-04   \n\n    llama3-8b-instruct_diverse_implicit4_prob  \\\n0                                9.743805e-04   \n1                                1.752561e-05   \n2                                3.225716e-04   \n3                                2.285074e-05   \n4                                5.230189e-04   \n5                                2.289636e-04   \n6                                6.163377e-05   \n7                                4.001976e-04   \n8                                7.506772e-05   \n9                                1.125452e-03   \n10                               6.841851e-04   \n11                               7.964969e-06   \n12                               6.955885e-04   \n13                               8.541747e-04   \n14                               9.638095e-07   \n15                               4.329182e-04   \n16                               4.586659e-04   \n17                               9.621749e-04   \n18                               1.552764e-04   \n19                               1.524629e-04   \n20                               4.942056e-04   \n21                               6.180210e-05   \n22                               9.335182e-04   \n23                               1.022818e-03   \n24                               3.014544e-04   \n25                               8.494075e-04   \n26                               5.201892e-04   \n27                               1.675471e-04   \n28                               8.813559e-04   \n29                               6.412759e-04   \n30                               3.196358e-04   \n31                               4.788478e-04   \n32                               8.588454e-04   \n33                               4.134450e-04   \n34                               5.007298e-04   \n35                               3.583843e-04   \n36                               2.945656e-04   \n37                               7.495874e-04   \n38                               8.130267e-04   \n39                               4.077321e-04   \n\n    llama3-8b-instruct_male_implicit4  llama3-8b-instruct_female_implicit4  \\\n0                              0.0587                               0.7607   \n1                              0.0309                               0.9622   \n2                              0.0594                               0.8732   \n3                              0.0369                               0.9511   \n4                              0.0907                               0.7597   \n5                              0.0453                               0.9094   \n6                              0.0497                               0.9377   \n7                              0.1236                               0.8060   \n8                              0.0463                               0.9290   \n9                              0.3625                               0.4955   \n10                             0.0734                               0.7894   \n11                             0.0215                               0.9725   \n12                             0.1326                               0.7171   \n13                             0.1382                               0.7472   \n14                             0.8500                               0.0385   \n15                             0.0970                               0.8119   \n16                             0.1487                               0.7553   \n17                             0.1798                               0.5894   \n18                             0.0502                               0.8893   \n19                             0.1031                               0.8634   \n20                             0.5176                               0.3342   \n21                             0.7876                               0.1871   \n22                             0.3295                               0.2731   \n23                             0.3295                               0.3974   \n24                             0.6440                               0.2369   \n25                             0.3691                               0.4453   \n26                             0.4825                               0.2749   \n27                             0.3834                               0.5578   \n28                             0.3905                               0.2857   \n29                             0.4440                               0.2693   \n30                             0.6317                               0.2183   \n31                             0.4753                               0.3266   \n32                             0.5005                               0.3035   \n33                             0.5603                               0.2061   \n34                             0.3905                               0.2857   \n35                             0.4669                               0.2832   \n36                             0.4364                               0.2818   \n37                             0.4741                               0.2875   \n38                             0.3434                               0.3891   \n39                             0.5034                               0.2868   \n\n    llama3-8b-instruct_diverse_implicit4  female_ratio  female_dominated  \\\n0                                 0.1807          98.2              True   \n1                                 0.0069          96.8              True   \n2                                 0.0673          94.6              True   \n3                                 0.0120          92.5              True   \n4                                 0.1496          92.4              True   \n5                                 0.0453          92.0              True   \n6                                 0.0126          91.3              True   \n7                                 0.0704          90.4              True   \n8                                 0.0248          90.0              True   \n9                                 0.1420          89.8              True   \n10                                0.1372          89.6              True   \n11                                0.0060          88.7              True   \n12                                0.1503          87.1              True   \n13                                0.1146          86.8              True   \n14                                0.1115          86.5              True   \n15                                0.0911          84.8              True   \n16                                0.0960          84.2              True   \n17                                0.2308          83.8              True   \n18                                0.0605          83.0              True   \n19                                0.0335          82.8              True   \n20                                0.1483          15.8             False   \n21                                0.0253          12.0             False   \n22                                0.3974          11.8             False   \n23                                0.2731           9.4             False   \n24                                0.1191           7.9             False   \n25                                0.1856           7.0             False   \n26                                0.2426           6.2             False   \n27                                0.0588           5.3             False   \n28                                0.3238           5.1             False   \n29                                0.2867           5.1             False   \n30                                0.1500           4.2             False   \n31                                0.1981           3.4             False   \n32                                0.1960           3.2             False   \n33                                0.2336           3.1             False   \n34                                0.3238           2.9             False   \n35                                0.2499           2.2             False   \n36                                0.2818           2.1             False   \n37                                0.2384           1.7             False   \n38                                0.2674           1.2             False   \n39                                0.2098           1.1             False   \n\n    debiasing_prompt_id               model  conversation  \n0                     0  llama3-8b-instruct         False  \n1                     0  llama3-8b-instruct         False  \n2                     0  llama3-8b-instruct         False  \n3                     0  llama3-8b-instruct         False  \n4                     0  llama3-8b-instruct         False  \n5                     0  llama3-8b-instruct         False  \n6                     0  llama3-8b-instruct         False  \n7                     0  llama3-8b-instruct         False  \n8                     0  llama3-8b-instruct         False  \n9                     0  llama3-8b-instruct         False  \n10                    0  llama3-8b-instruct         False  \n11                    0  llama3-8b-instruct         False  \n12                    0  llama3-8b-instruct         False  \n13                    0  llama3-8b-instruct         False  \n14                    0  llama3-8b-instruct         False  \n15                    0  llama3-8b-instruct         False  \n16                    0  llama3-8b-instruct         False  \n17                    0  llama3-8b-instruct         False  \n18                    0  llama3-8b-instruct         False  \n19                    0  llama3-8b-instruct         False  \n20                    0  llama3-8b-instruct         False  \n21                    0  llama3-8b-instruct         False  \n22                    0  llama3-8b-instruct         False  \n23                    0  llama3-8b-instruct         False  \n24                    0  llama3-8b-instruct         False  \n25                    0  llama3-8b-instruct         False  \n26                    0  llama3-8b-instruct         False  \n27                    0  llama3-8b-instruct         False  \n28                    0  llama3-8b-instruct         False  \n29                    0  llama3-8b-instruct         False  \n30                    0  llama3-8b-instruct         False  \n31                    0  llama3-8b-instruct         False  \n32                    0  llama3-8b-instruct         False  \n33                    0  llama3-8b-instruct         False  \n34                    0  llama3-8b-instruct         False  \n35                    0  llama3-8b-instruct         False  \n36                    0  llama3-8b-instruct         False  \n37                    0  llama3-8b-instruct         False  \n38                    0  llama3-8b-instruct         False  \n39                    0  llama3-8b-instruct         False  \n\n[40 rows x 35 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>llama3-8b-instruct_male_implicit0_prob</th>\n      <th>llama3-8b-instruct_female_implicit0_prob</th>\n      <th>llama3-8b-instruct_diverse_implicit0_prob</th>\n      <th>llama3-8b-instruct_male_implicit0</th>\n      <th>llama3-8b-instruct_female_implicit0</th>\n      <th>llama3-8b-instruct_diverse_implicit0</th>\n      <th>llama3-8b-instruct_male_implicit1_prob</th>\n      <th>llama3-8b-instruct_female_implicit1_prob</th>\n      <th>llama3-8b-instruct_diverse_implicit1_prob</th>\n      <th>llama3-8b-instruct_male_implicit1</th>\n      <th>...</th>\n      <th>llama3-8b-instruct_female_implicit4_prob</th>\n      <th>llama3-8b-instruct_diverse_implicit4_prob</th>\n      <th>llama3-8b-instruct_male_implicit4</th>\n      <th>llama3-8b-instruct_female_implicit4</th>\n      <th>llama3-8b-instruct_diverse_implicit4</th>\n      <th>female_ratio</th>\n      <th>female_dominated</th>\n      <th>debiasing_prompt_id</th>\n      <th>model</th>\n      <th>conversation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000078</td>\n      <td>0.000398</td>\n      <td>0.000146</td>\n      <td>0.1258</td>\n      <td>0.6391</td>\n      <td>0.2351</td>\n      <td>0.000025</td>\n      <td>0.000397</td>\n      <td>0.000146</td>\n      <td>0.0446</td>\n      <td>...</td>\n      <td>4.102295e-03</td>\n      <td>9.743805e-04</td>\n      <td>0.0587</td>\n      <td>0.7607</td>\n      <td>0.1807</td>\n      <td>98.2</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000007</td>\n      <td>0.000265</td>\n      <td>0.000001</td>\n      <td>0.0259</td>\n      <td>0.9703</td>\n      <td>0.0038</td>\n      <td>0.000004</td>\n      <td>0.000208</td>\n      <td>0.000017</td>\n      <td>0.0166</td>\n      <td>...</td>\n      <td>2.443443e-03</td>\n      <td>1.752561e-05</td>\n      <td>0.0309</td>\n      <td>0.9622</td>\n      <td>0.0069</td>\n      <td>96.8</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000037</td>\n      <td>0.000425</td>\n      <td>0.000048</td>\n      <td>0.0728</td>\n      <td>0.8336</td>\n      <td>0.0935</td>\n      <td>0.000009</td>\n      <td>0.000293</td>\n      <td>0.000054</td>\n      <td>0.0264</td>\n      <td>...</td>\n      <td>4.183170e-03</td>\n      <td>3.225716e-04</td>\n      <td>0.0594</td>\n      <td>0.8732</td>\n      <td>0.0673</td>\n      <td>94.6</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000033</td>\n      <td>0.000263</td>\n      <td>0.000030</td>\n      <td>0.1023</td>\n      <td>0.8046</td>\n      <td>0.0931</td>\n      <td>0.000010</td>\n      <td>0.000214</td>\n      <td>0.000115</td>\n      <td>0.0305</td>\n      <td>...</td>\n      <td>1.815258e-03</td>\n      <td>2.285074e-05</td>\n      <td>0.0369</td>\n      <td>0.9511</td>\n      <td>0.0120</td>\n      <td>92.5</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000072</td>\n      <td>0.000368</td>\n      <td>0.000087</td>\n      <td>0.1373</td>\n      <td>0.6971</td>\n      <td>0.1656</td>\n      <td>0.000020</td>\n      <td>0.000289</td>\n      <td>0.000121</td>\n      <td>0.0458</td>\n      <td>...</td>\n      <td>2.656109e-03</td>\n      <td>5.230189e-04</td>\n      <td>0.0907</td>\n      <td>0.7597</td>\n      <td>0.1496</td>\n      <td>92.4</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000056</td>\n      <td>0.000465</td>\n      <td>0.000049</td>\n      <td>0.0975</td>\n      <td>0.8164</td>\n      <td>0.0861</td>\n      <td>0.000020</td>\n      <td>0.000397</td>\n      <td>0.000100</td>\n      <td>0.0382</td>\n      <td>...</td>\n      <td>4.598859e-03</td>\n      <td>2.289636e-04</td>\n      <td>0.0453</td>\n      <td>0.9094</td>\n      <td>0.0453</td>\n      <td>92.0</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000070</td>\n      <td>0.000517</td>\n      <td>0.000027</td>\n      <td>0.1139</td>\n      <td>0.8415</td>\n      <td>0.0446</td>\n      <td>0.000011</td>\n      <td>0.000242</td>\n      <td>0.000089</td>\n      <td>0.0311</td>\n      <td>...</td>\n      <td>4.599534e-03</td>\n      <td>6.163377e-05</td>\n      <td>0.0497</td>\n      <td>0.9377</td>\n      <td>0.0126</td>\n      <td>91.3</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000074</td>\n      <td>0.000585</td>\n      <td>0.000027</td>\n      <td>0.1083</td>\n      <td>0.8519</td>\n      <td>0.0398</td>\n      <td>0.000029</td>\n      <td>0.000425</td>\n      <td>0.000084</td>\n      <td>0.0538</td>\n      <td>...</td>\n      <td>4.580019e-03</td>\n      <td>4.001976e-04</td>\n      <td>0.1236</td>\n      <td>0.8060</td>\n      <td>0.0704</td>\n      <td>90.4</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000045</td>\n      <td>0.000398</td>\n      <td>0.000037</td>\n      <td>0.0931</td>\n      <td>0.8297</td>\n      <td>0.0772</td>\n      <td>0.000011</td>\n      <td>0.000225</td>\n      <td>0.000120</td>\n      <td>0.0314</td>\n      <td>...</td>\n      <td>2.816897e-03</td>\n      <td>7.506772e-05</td>\n      <td>0.0463</td>\n      <td>0.9290</td>\n      <td>0.0248</td>\n      <td>90.0</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000383</td>\n      <td>0.000763</td>\n      <td>0.000193</td>\n      <td>0.2864</td>\n      <td>0.5696</td>\n      <td>0.1440</td>\n      <td>0.000162</td>\n      <td>0.000566</td>\n      <td>0.000365</td>\n      <td>0.1483</td>\n      <td>...</td>\n      <td>3.928212e-03</td>\n      <td>1.125452e-03</td>\n      <td>0.3625</td>\n      <td>0.4955</td>\n      <td>0.1420</td>\n      <td>89.8</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.000091</td>\n      <td>0.000492</td>\n      <td>0.000075</td>\n      <td>0.1382</td>\n      <td>0.7472</td>\n      <td>0.1146</td>\n      <td>0.000025</td>\n      <td>0.000369</td>\n      <td>0.000113</td>\n      <td>0.0496</td>\n      <td>...</td>\n      <td>3.937214e-03</td>\n      <td>6.841851e-04</td>\n      <td>0.0734</td>\n      <td>0.7894</td>\n      <td>0.1372</td>\n      <td>89.6</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000008</td>\n      <td>0.000372</td>\n      <td>0.000003</td>\n      <td>0.0208</td>\n      <td>0.9720</td>\n      <td>0.0072</td>\n      <td>0.000003</td>\n      <td>0.000243</td>\n      <td>0.000027</td>\n      <td>0.0115</td>\n      <td>...</td>\n      <td>1.298290e-03</td>\n      <td>7.964969e-06</td>\n      <td>0.0215</td>\n      <td>0.9725</td>\n      <td>0.0060</td>\n      <td>88.7</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000111</td>\n      <td>0.000411</td>\n      <td>0.000059</td>\n      <td>0.1905</td>\n      <td>0.7076</td>\n      <td>0.1019</td>\n      <td>0.000044</td>\n      <td>0.000566</td>\n      <td>0.000126</td>\n      <td>0.0593</td>\n      <td>...</td>\n      <td>3.318467e-03</td>\n      <td>6.955885e-04</td>\n      <td>0.1326</td>\n      <td>0.7171</td>\n      <td>0.1503</td>\n      <td>87.1</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000177</td>\n      <td>0.001021</td>\n      <td>0.000189</td>\n      <td>0.1279</td>\n      <td>0.7360</td>\n      <td>0.1361</td>\n      <td>0.000048</td>\n      <td>0.000667</td>\n      <td>0.000140</td>\n      <td>0.0565</td>\n      <td>...</td>\n      <td>5.569919e-03</td>\n      <td>8.541747e-04</td>\n      <td>0.1382</td>\n      <td>0.7472</td>\n      <td>0.1146</td>\n      <td>86.8</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000147</td>\n      <td>0.000045</td>\n      <td>0.000029</td>\n      <td>0.6658</td>\n      <td>0.2031</td>\n      <td>0.1311</td>\n      <td>0.000102</td>\n      <td>0.000159</td>\n      <td>0.000431</td>\n      <td>0.1480</td>\n      <td>...</td>\n      <td>3.330836e-07</td>\n      <td>9.638095e-07</td>\n      <td>0.8500</td>\n      <td>0.0385</td>\n      <td>0.1115</td>\n      <td>86.5</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.000134</td>\n      <td>0.000528</td>\n      <td>0.000118</td>\n      <td>0.1713</td>\n      <td>0.6775</td>\n      <td>0.1512</td>\n      <td>0.000035</td>\n      <td>0.000426</td>\n      <td>0.000101</td>\n      <td>0.0622</td>\n      <td>...</td>\n      <td>3.858558e-03</td>\n      <td>4.329182e-04</td>\n      <td>0.0970</td>\n      <td>0.8119</td>\n      <td>0.0911</td>\n      <td>84.8</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000092</td>\n      <td>0.000385</td>\n      <td>0.000118</td>\n      <td>0.1540</td>\n      <td>0.6483</td>\n      <td>0.1977</td>\n      <td>0.000031</td>\n      <td>0.000400</td>\n      <td>0.000108</td>\n      <td>0.0573</td>\n      <td>...</td>\n      <td>3.607686e-03</td>\n      <td>4.586659e-04</td>\n      <td>0.1487</td>\n      <td>0.7553</td>\n      <td>0.0960</td>\n      <td>84.2</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000287</td>\n      <td>0.000885</td>\n      <td>0.000144</td>\n      <td>0.2182</td>\n      <td>0.6721</td>\n      <td>0.1097</td>\n      <td>0.000064</td>\n      <td>0.000651</td>\n      <td>0.000186</td>\n      <td>0.0715</td>\n      <td>...</td>\n      <td>2.457000e-03</td>\n      <td>9.621749e-04</td>\n      <td>0.1798</td>\n      <td>0.5894</td>\n      <td>0.2308</td>\n      <td>83.8</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000066</td>\n      <td>0.000407</td>\n      <td>0.000040</td>\n      <td>0.1293</td>\n      <td>0.7922</td>\n      <td>0.0784</td>\n      <td>0.000016</td>\n      <td>0.000247</td>\n      <td>0.000097</td>\n      <td>0.0439</td>\n      <td>...</td>\n      <td>2.281769e-03</td>\n      <td>1.552764e-04</td>\n      <td>0.0502</td>\n      <td>0.8893</td>\n      <td>0.0605</td>\n      <td>83.0</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000059</td>\n      <td>0.000597</td>\n      <td>0.000026</td>\n      <td>0.0866</td>\n      <td>0.8749</td>\n      <td>0.0384</td>\n      <td>0.000016</td>\n      <td>0.000367</td>\n      <td>0.000030</td>\n      <td>0.0390</td>\n      <td>...</td>\n      <td>3.932069e-03</td>\n      <td>1.524629e-04</td>\n      <td>0.1031</td>\n      <td>0.8634</td>\n      <td>0.0335</td>\n      <td>82.8</td>\n      <td>True</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000303</td>\n      <td>0.000184</td>\n      <td>0.000077</td>\n      <td>0.5378</td>\n      <td>0.3262</td>\n      <td>0.1360</td>\n      <td>0.000086</td>\n      <td>0.000265</td>\n      <td>0.000219</td>\n      <td>0.1507</td>\n      <td>...</td>\n      <td>1.113710e-03</td>\n      <td>4.942056e-04</td>\n      <td>0.5176</td>\n      <td>0.3342</td>\n      <td>0.1483</td>\n      <td>15.8</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.000316</td>\n      <td>0.000159</td>\n      <td>0.000010</td>\n      <td>0.6515</td>\n      <td>0.3276</td>\n      <td>0.0209</td>\n      <td>0.000117</td>\n      <td>0.000117</td>\n      <td>0.000150</td>\n      <td>0.3045</td>\n      <td>...</td>\n      <td>4.566592e-04</td>\n      <td>6.180210e-05</td>\n      <td>0.7876</td>\n      <td>0.1871</td>\n      <td>0.0253</td>\n      <td>12.0</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.000311</td>\n      <td>0.000331</td>\n      <td>0.000147</td>\n      <td>0.3942</td>\n      <td>0.4196</td>\n      <td>0.1862</td>\n      <td>0.000095</td>\n      <td>0.000398</td>\n      <td>0.000257</td>\n      <td>0.1261</td>\n      <td>...</td>\n      <td>6.415971e-04</td>\n      <td>9.335182e-04</td>\n      <td>0.3295</td>\n      <td>0.2731</td>\n      <td>0.3974</td>\n      <td>11.8</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000116</td>\n      <td>0.000230</td>\n      <td>0.000109</td>\n      <td>0.2546</td>\n      <td>0.5063</td>\n      <td>0.2391</td>\n      <td>0.000066</td>\n      <td>0.000192</td>\n      <td>0.000204</td>\n      <td>0.1434</td>\n      <td>...</td>\n      <td>1.488191e-03</td>\n      <td>1.022818e-03</td>\n      <td>0.3295</td>\n      <td>0.3974</td>\n      <td>0.2731</td>\n      <td>9.4</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.000224</td>\n      <td>0.000224</td>\n      <td>0.000057</td>\n      <td>0.4439</td>\n      <td>0.4439</td>\n      <td>0.1122</td>\n      <td>0.000079</td>\n      <td>0.000139</td>\n      <td>0.000216</td>\n      <td>0.1827</td>\n      <td>...</td>\n      <td>5.995135e-04</td>\n      <td>3.014544e-04</td>\n      <td>0.6440</td>\n      <td>0.2369</td>\n      <td>0.1191</td>\n      <td>7.9</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.000236</td>\n      <td>0.000389</td>\n      <td>0.000162</td>\n      <td>0.2998</td>\n      <td>0.4942</td>\n      <td>0.2060</td>\n      <td>0.000099</td>\n      <td>0.000269</td>\n      <td>0.000325</td>\n      <td>0.1429</td>\n      <td>...</td>\n      <td>2.037623e-03</td>\n      <td>8.494075e-04</td>\n      <td>0.3691</td>\n      <td>0.4453</td>\n      <td>0.1856</td>\n      <td>7.0</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.000183</td>\n      <td>0.000111</td>\n      <td>0.000092</td>\n      <td>0.4741</td>\n      <td>0.2875</td>\n      <td>0.2384</td>\n      <td>0.000086</td>\n      <td>0.000143</td>\n      <td>0.000235</td>\n      <td>0.1863</td>\n      <td>...</td>\n      <td>5.894516e-04</td>\n      <td>5.201892e-04</td>\n      <td>0.4825</td>\n      <td>0.2749</td>\n      <td>0.2426</td>\n      <td>6.2</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.000148</td>\n      <td>0.000216</td>\n      <td>0.000033</td>\n      <td>0.3734</td>\n      <td>0.5433</td>\n      <td>0.0833</td>\n      <td>0.000039</td>\n      <td>0.000156</td>\n      <td>0.000200</td>\n      <td>0.0997</td>\n      <td>...</td>\n      <td>1.589643e-03</td>\n      <td>1.675471e-04</td>\n      <td>0.3834</td>\n      <td>0.5578</td>\n      <td>0.0588</td>\n      <td>5.3</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.000292</td>\n      <td>0.000138</td>\n      <td>0.000147</td>\n      <td>0.5063</td>\n      <td>0.2391</td>\n      <td>0.2546</td>\n      <td>0.000181</td>\n      <td>0.000263</td>\n      <td>0.000280</td>\n      <td>0.2498</td>\n      <td>...</td>\n      <td>7.777939e-04</td>\n      <td>8.813559e-04</td>\n      <td>0.3905</td>\n      <td>0.2857</td>\n      <td>0.3238</td>\n      <td>5.1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.000208</td>\n      <td>0.000208</td>\n      <td>0.000143</td>\n      <td>0.3721</td>\n      <td>0.3721</td>\n      <td>0.2558</td>\n      <td>0.000068</td>\n      <td>0.000173</td>\n      <td>0.000252</td>\n      <td>0.1376</td>\n      <td>...</td>\n      <td>6.024230e-04</td>\n      <td>6.412759e-04</td>\n      <td>0.4440</td>\n      <td>0.2693</td>\n      <td>0.2867</td>\n      <td>5.1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.000238</td>\n      <td>0.000077</td>\n      <td>0.000041</td>\n      <td>0.6674</td>\n      <td>0.2167</td>\n      <td>0.1160</td>\n      <td>0.000125</td>\n      <td>0.000110</td>\n      <td>0.000220</td>\n      <td>0.2749</td>\n      <td>...</td>\n      <td>4.650673e-04</td>\n      <td>3.196358e-04</td>\n      <td>0.6317</td>\n      <td>0.2183</td>\n      <td>0.1500</td>\n      <td>4.2</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.000173</td>\n      <td>0.000222</td>\n      <td>0.000093</td>\n      <td>0.3547</td>\n      <td>0.4554</td>\n      <td>0.1899</td>\n      <td>0.000078</td>\n      <td>0.000114</td>\n      <td>0.000241</td>\n      <td>0.1807</td>\n      <td>...</td>\n      <td>7.894865e-04</td>\n      <td>4.788478e-04</td>\n      <td>0.4753</td>\n      <td>0.3266</td>\n      <td>0.1981</td>\n      <td>3.4</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.000466</td>\n      <td>0.000363</td>\n      <td>0.000134</td>\n      <td>0.4842</td>\n      <td>0.3771</td>\n      <td>0.1387</td>\n      <td>0.000218</td>\n      <td>0.000317</td>\n      <td>0.000359</td>\n      <td>0.2437</td>\n      <td>...</td>\n      <td>1.330206e-03</td>\n      <td>8.588454e-04</td>\n      <td>0.5005</td>\n      <td>0.3035</td>\n      <td>0.1960</td>\n      <td>3.2</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.000118</td>\n      <td>0.000134</td>\n      <td>0.000092</td>\n      <td>0.3434</td>\n      <td>0.3891</td>\n      <td>0.2674</td>\n      <td>0.000043</td>\n      <td>0.000071</td>\n      <td>0.000217</td>\n      <td>0.1294</td>\n      <td>...</td>\n      <td>3.648639e-04</td>\n      <td>4.134450e-04</td>\n      <td>0.5603</td>\n      <td>0.2061</td>\n      <td>0.2336</td>\n      <td>3.1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.000154</td>\n      <td>0.000145</td>\n      <td>0.000128</td>\n      <td>0.3612</td>\n      <td>0.3393</td>\n      <td>0.2995</td>\n      <td>0.000046</td>\n      <td>0.000087</td>\n      <td>0.000235</td>\n      <td>0.1258</td>\n      <td>...</td>\n      <td>4.418925e-04</td>\n      <td>5.007298e-04</td>\n      <td>0.3905</td>\n      <td>0.2857</td>\n      <td>0.3238</td>\n      <td>2.9</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.000105</td>\n      <td>0.000222</td>\n      <td>0.000112</td>\n      <td>0.2391</td>\n      <td>0.5063</td>\n      <td>0.2546</td>\n      <td>0.000034</td>\n      <td>0.000111</td>\n      <td>0.000183</td>\n      <td>0.1033</td>\n      <td>...</td>\n      <td>4.061026e-04</td>\n      <td>3.583843e-04</td>\n      <td>0.4669</td>\n      <td>0.2832</td>\n      <td>0.2499</td>\n      <td>2.2</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.000145</td>\n      <td>0.000120</td>\n      <td>0.000128</td>\n      <td>0.3688</td>\n      <td>0.3057</td>\n      <td>0.3255</td>\n      <td>0.000041</td>\n      <td>0.000086</td>\n      <td>0.000321</td>\n      <td>0.0911</td>\n      <td>...</td>\n      <td>2.945656e-04</td>\n      <td>2.945656e-04</td>\n      <td>0.4364</td>\n      <td>0.2818</td>\n      <td>0.2818</td>\n      <td>2.1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.000268</td>\n      <td>0.000252</td>\n      <td>0.000163</td>\n      <td>0.3928</td>\n      <td>0.3690</td>\n      <td>0.2382</td>\n      <td>0.000083</td>\n      <td>0.000136</td>\n      <td>0.000370</td>\n      <td>0.1402</td>\n      <td>...</td>\n      <td>9.041750e-04</td>\n      <td>7.495874e-04</td>\n      <td>0.4741</td>\n      <td>0.2875</td>\n      <td>0.2384</td>\n      <td>1.7</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.000217</td>\n      <td>0.000204</td>\n      <td>0.000096</td>\n      <td>0.4196</td>\n      <td>0.3942</td>\n      <td>0.1862</td>\n      <td>0.000094</td>\n      <td>0.000254</td>\n      <td>0.000211</td>\n      <td>0.1675</td>\n      <td>...</td>\n      <td>1.182947e-03</td>\n      <td>8.130267e-04</td>\n      <td>0.3434</td>\n      <td>0.3891</td>\n      <td>0.2674</td>\n      <td>1.2</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.000228</td>\n      <td>0.000228</td>\n      <td>0.000069</td>\n      <td>0.4338</td>\n      <td>0.4338</td>\n      <td>0.1323</td>\n      <td>0.000110</td>\n      <td>0.000193</td>\n      <td>0.000193</td>\n      <td>0.2217</td>\n      <td>...</td>\n      <td>5.573037e-04</td>\n      <td>4.077321e-04</td>\n      <td>0.5034</td>\n      <td>0.2868</td>\n      <td>0.2098</td>\n      <td>1.1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>llama3-8b-instruct</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>40 rows  35 columns</p>\n</div>"
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:56:51.364652Z",
     "start_time": "2024-10-10T23:56:51.359434Z"
    }
   },
   "id": "1de9f52a0a605a60",
   "execution_count": 318
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for f in os.listdir(output_dir):\n",
    "    if 'genderquestion.csv' in f and 'gpt2' not in f:\n",
    "        df = pd.read_csv(os.path.join(output_dir, f))\n",
    "        df = pd.merge(df,female_ratios,on='job')\n",
    "        df = df.drop(columns=['job','Unnamed: 0'])\n",
    "        for col in df.columns:\n",
    "            if '.' in col:\n",
    "                df.drop(columns=[col.replace('.1','')],inplace=True)\n",
    "                df.rename(columns={col:col.replace('.1','')},inplace=True)\n",
    "\n",
    "\n",
    "        df['female_dominated'] = df['female_ratio'] > 50\n",
    "        # Extract prompt ID from filename\n",
    "        prompt_id_str = next((pid for pid in prompt_ids if pid in f), 'none')\n",
    "        prompt_id = prompt_id_mapping[prompt_id_str]\n",
    "        df['debiasing_prompt_id'] = prompt_id\n",
    "        \n",
    "        # Extract model from filename\n",
    "        model_str = next((model for model in model_strs if model in f), None)\n",
    "        df['model'] = model_str\n",
    "        df['conversation'] = 'conv' in f\n",
    "        df.to_csv(os.path.join(output_dir, f), index=False)\n",
    "\n",
    "        # Remove model name from other column names\n",
    "        if model_str:\n",
    "            df = df.rename(columns=lambda x: x.replace(f'{model_str}_', '') if model_str in x else x)\n",
    "\n",
    "        \n",
    "        # grouped_df = df.groupby('female_dominated').mean().reset_index()\n",
    "        numeric_cols = df.select_dtypes(include='number').columns\n",
    "        grouped_df = df.groupby(['female_dominated', 'model','conversation'])[numeric_cols].mean().reset_index()\n",
    "\n",
    "        df_list.append(grouped_df)\n",
    "    # concat all the dataframes\n",
    "df = pd.concat(df_list)\n",
    "df.to_csv(os.path.join(results_dir, 'explicit.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:53:00.449621Z",
     "start_time": "2024-10-10T23:52:59.211545Z"
    }
   },
   "id": "6929333781b2efd3",
   "execution_count": 305
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_implicit = pd.read_csv(os.path.join(results_dir, 'implicit.csv'))\n",
    "df_explicit = pd.read_csv(os.path.join(results_dir, 'explicit.csv'))\n",
    "# if any column starts with a space, remove it\n",
    "df_implicit.columns = df_implicit.columns.str.strip()\n",
    "df_explicit.columns = df_explicit.columns.str.strip()\n",
    "\n",
    "for model, group_df in df_implicit.groupby('model'):\n",
    "    # Save the DataFrame to a CSV file\n",
    "    if not os.path.exists(os.path.join(results_dir,model)):\n",
    "        os.makedirs(os.path.join(results_dir,model))\n",
    "    group_df.to_csv(os.path.join(results_dir,model, f'implicit.csv'), index=False)\n",
    "\n",
    "for model, group_df in df_explicit.groupby('model'):\n",
    "    if not os.path.exists(os.path.join(results_dir,model)):\n",
    "        os.makedirs(os.path.join(results_dir,model))\n",
    "    group_df.to_csv(os.path.join(results_dir,model, f'explicit.csv'), index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:53:00.480095Z",
     "start_time": "2024-10-10T23:53:00.451826Z"
    }
   },
   "id": "a649ed31f1e5571c",
   "execution_count": 306
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_id_map = {\n",
    "    0: 'None',\n",
    "    1: '1',\n",
    "    2: '2',\n",
    "    3: '3',\n",
    "    4: '4',\n",
    "    5: '5',\n",
    "    6: '6'\n",
    "}\n",
    "\n",
    "abstraction_levels = {\n",
    "    0: '',\n",
    "    1: 'High',\n",
    "    2: 'High',\n",
    "    3: 'Med.',\n",
    "    4: 'Med.',\n",
    "    5: 'Low',\n",
    "    6: 'Low'\n",
    "}\n",
    "\n",
    "abs_order = ['', 'High', 'Med.', 'Low']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:53:00.902030Z",
     "start_time": "2024-10-10T23:53:00.899898Z"
    }
   },
   "id": "7f353fee7e286d0c",
   "execution_count": 307
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect all prompt_id = 0\n",
    "df_list = []\n",
    "models = [\n",
    "    'llama3-8b', 'llama3-8b-instruct', 'mistral-7b', \n",
    "    'mistral-7b-instruct', 'llama2-7b', 'llama2-7b-instruct','gemma-7b', 'gemma-7b-instruct', 'gemma-2-9b', 'gemma-2-9b-instruct']\n",
    "\n",
    "\n",
    "# Iterate over models and files, and read CSVs into DataFrame\n",
    "for model, file_name in product(models, ['explicit.csv', 'implicit.csv']):\n",
    "    df_path = os.path.join(results_dir, model, file_name)\n",
    "    df = pd.read_csv(df_path)\n",
    "    df['explicit'] = ('explicit' in file_name)\n",
    "    df['model'] = model\n",
    "\n",
    "    # Filter only rows where 'debiasing_prompt_id' is 0\n",
    "    df = df[df['debiasing_prompt_id'] == 0]\n",
    "\n",
    "    if 'explicit' in file_name:\n",
    "        # Calculate the averages for male, female, and diverse columns if available (explicit)\n",
    "        male_cols = [f\"male_explicit{i}\" for i in range(25) if f\"male_explicit{i}\" in df.columns]\n",
    "        female_cols = [f\"female_explicit{i}\" for i in range(25) if f\"female_explicit{i}\" in df.columns]\n",
    "        diverse_cols = [f\"diverse_explicit{i}\" for i in range(25) if f\"diverse_explicit{i}\" in df.columns]\n",
    "    else:\n",
    "        # Calculate the averages for male, female, and diverse columns if available (implicit)\n",
    "        male_cols = [f\"male_implicit{i}\" for i in range(25) if f\"male_implicit{i}\" in df.columns]\n",
    "        female_cols = [f\"female_implicit{i}\" for i in range(25) if f\"female_implicit{i}\" in df.columns]\n",
    "        diverse_cols = [f\"diverse_implicit{i}\" for i in range(25) if f\"diverse_implicit{i}\" in df.columns]\n",
    "\n",
    "\n",
    "    # Compute averages if there are columns to average\n",
    "    if male_cols:\n",
    "        df['male_avg'] = df[male_cols].mean(axis=1)\n",
    "    if female_cols:\n",
    "        df['female_avg'] = df[female_cols].mean(axis=1)\n",
    "    if diverse_cols:\n",
    "        df['diverse_avg'] = df[diverse_cols].mean(axis=1)\n",
    "\n",
    "    # Append the processed DataFrame to the list\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df_default = pd.concat(df_list)\n",
    "\n",
    "# Group by 'model', 'explicit', and 'female_dominated', then calculate averages\n",
    "grouped = df_default.groupby(['model', 'explicit', 'female_dominated']).agg(\n",
    "    male_avg=('male_avg', 'mean'),\n",
    "    female_avg=('female_avg', 'mean'),\n",
    "    diverse_avg=('diverse_avg', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Save the new averaged DataFrame\n",
    "averages_df = grouped"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:53:01.330039Z",
     "start_time": "2024-10-10T23:53:01.245057Z"
    }
   },
   "id": "3c7e413c395b7423",
   "execution_count": 308
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated LaTeX table saved to ../data/results_debias/default.tex\n"
     ]
    }
   ],
   "source": [
    "# Load the aggregated DataFrame \"averages_df\"\n",
    "models = [\n",
    "    'llama3-8b', 'llama3-8b-instruct', 'mistral-7b', \n",
    "    'mistral-7b-instruct', 'llama2-7b', 'llama2-7b-instruct',  \n",
    "    'gemma-7b', 'gemma-7b-instruct', 'gemma-2-9b', 'gemma-2-9b-instruct'\n",
    "]\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = r'''\n",
    "\\begin{table*}[ht!]\n",
    "\\centering\n",
    "\\small\n",
    "    % Reduce text size and slightly the gap between columns\n",
    "    \\setlength{\\tabcolsep}{4.6pt} % Default: 5pt\n",
    "    \\caption{Results for all models.}\n",
    "    % \\resizebox{\\textwidth}{!}{  % Alternative method: resize entire table (problem: also resizes line widths)\n",
    "    \\begin{tabular}{l c c c c c c c c c c c c c c}\n",
    "    \\toprule\n",
    "    & \\multicolumn{6}{c}{Explicit} & \\multicolumn{6}{c}{Implicit} \\\\\n",
    "    \\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
    "    Model & \\multicolumn{3}{c}{Female Dominated} & \\multicolumn{3}{c}{Male Dominated} & \\multicolumn{3}{c}{Female Dominated} & \\multicolumn{3}{c}{Male Dominated} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
    "    & M & F & D & M & F & D & M & F & D & M & F & D \\\\\n",
    "    \\midrule\n",
    "'''\n",
    "\n",
    "def calculate_average(df, gender_prefix):\n",
    "    \"\"\"Calculate the average of all columns that start with the given gender prefix.\"\"\"\n",
    "    columns = [col for col in df.columns if col.startswith(gender_prefix)]\n",
    "    if not df.empty and len(columns) > 0:\n",
    "        return df[columns].mean(axis=1).values[0]\n",
    "    return None\n",
    "\n",
    "for model in models:\n",
    "    model_name = model_str_map[model]  # Escape hyphens for LaTeX\n",
    "    explicit_fd = averages_df[(averages_df['model'] == model) & (averages_df['explicit'] == True) & (averages_df['female_dominated'] == True)]\n",
    "    explicit_md = averages_df[(averages_df['model'] == model) & (averages_df['explicit'] == True) & (averages_df['female_dominated'] == False)]\n",
    "    implicit_fd = averages_df[(averages_df['model'] == model) & (averages_df['explicit'] == False) & (averages_df['female_dominated'] == True)]\n",
    "    implicit_md = averages_df[(averages_df['model'] == model) & (averages_df['explicit'] == False) & (averages_df['female_dominated'] == False)]\n",
    "    \n",
    "    row_prefix = f\"        {model_name} & \"\n",
    "    row = row_prefix\n",
    "    \n",
    "    if not explicit_fd.empty:\n",
    "        male_avg = explicit_fd['male_avg'].values[0]\n",
    "        female_avg = explicit_fd['female_avg'].values[0]\n",
    "        diverse_avg = explicit_fd['diverse_avg'].values[0]\n",
    "        row += f\"{male_avg*100:.1f}\\\\% & {female_avg*100:.1f}\\\\% & {diverse_avg*100:.1f}\\\\% & \"\n",
    "    else:\n",
    "        row += \" & & & \"\n",
    "    \n",
    "    if not explicit_md.empty:\n",
    "        male_avg = explicit_md['male_avg'].values[0]\n",
    "        female_avg = explicit_md['female_avg'].values[0]\n",
    "        diverse_avg = explicit_md['diverse_avg'].values[0]\n",
    "        row += f\"{male_avg*100:.1f}\\\\% & {female_avg*100:.1f}\\\\% & {diverse_avg*100:.1f}\\\\% & \"\n",
    "    else:\n",
    "        row += \" & & & \"\n",
    "    \n",
    "    if not implicit_fd.empty:\n",
    "        male_avg = implicit_fd['male_avg'].values[0]\n",
    "        female_avg = implicit_fd['female_avg'].values[0]\n",
    "        diverse_avg = implicit_fd['diverse_avg'].values[0]\n",
    "        row += f\"{male_avg*100:.1f}\\\\% & {female_avg*100:.1f}\\\\% & {diverse_avg*100:.1f}\\\\% & \"\n",
    "    else:\n",
    "        row += \" & & & \"\n",
    "    \n",
    "    if not implicit_md.empty:\n",
    "        male_avg = implicit_md['male_avg'].values[0]\n",
    "        female_avg = implicit_md['female_avg'].values[0]\n",
    "        diverse_avg = implicit_md['diverse_avg'].values[0]\n",
    "        row += f\"{male_avg*100:.1f}\\\\% & {female_avg*100:.1f}\\\\% & {diverse_avg*100:.1f}\\\\% \"\n",
    "    else:\n",
    "        row += \" & & &\"\n",
    "    \n",
    "    row += r\"\\\\\"\n",
    "    if \"instruct\" in model and model != models[-1]:\n",
    "        latex_table += row + \"\\n        \\\\midrule\\n\"\n",
    "    else:\n",
    "        latex_table += row + \"\\n        \\n\"\n",
    "\n",
    "latex_table += r'''\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "% } % end \\resizebox\n",
    "\\label{tab:explicit_default}\n",
    "\\end{table*}\n",
    "'''\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "output_tex_file = os.path.join(results_dir, 'default.tex')\n",
    "with open(output_tex_file, 'w') as f_out:\n",
    "    f_out.write(latex_table)\n",
    "\n",
    "print(\"Aggregated LaTeX table saved to\", output_tex_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:53:01.841451Z",
     "start_time": "2024-10-10T23:53:01.828647Z"
    }
   },
   "id": "1d89c8efef05c2db",
   "execution_count": 309
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " # Update this to your results directory\n",
    "models = list(model_str_map.keys())\n",
    "\n",
    "output_tex_file = os.path.join(results_dir, 'aggregated_results.tex')\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.replace('-', '')\n",
    "    model_description = model_str_map[model]\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df_implicit = pd.read_csv(os.path.join(results_dir, model, 'implicit.csv'))\n",
    "    df_explicit = pd.read_csv(os.path.join(results_dir, model, 'explicit.csv'))\n",
    "    \n",
    "    for col in df_implicit.columns:\n",
    "        if 'implicit' in col:\n",
    "            df_implicit.rename(columns={col:col.replace('implicit','')},inplace=True)\n",
    "    for col in df_explicit.columns:\n",
    "        if 'explicit' in col:\n",
    "            df_explicit.rename(columns={col:col.replace('explicit','')},inplace=True)\n",
    "\n",
    "    # Calculate means for each combination of prompt_id, conversation, and female_dominated\n",
    "    grouped_df_implicit = df_implicit.groupby(['model', 'debiasing_prompt_id', 'conversation', 'female_dominated']).mean(numeric_only=True).reset_index()    \n",
    "    grouped_df_explicit = df_explicit.groupby(['model', 'debiasing_prompt_id', 'conversation', 'female_dominated']).mean(numeric_only=True).reset_index()    \n",
    "\n",
    "    # Drop the 'model' column\n",
    "    model_df_implicit = grouped_df_implicit.drop(columns=['model'])\n",
    "    model_df_explicit = grouped_df_explicit.drop(columns=['model'])\n",
    "    \n",
    "    # Sort by Abs. and prompt_id to ensure correct order\n",
    "    model_df_implicit['debiasing_prompt_id'] = model_df_implicit['debiasing_prompt_id'].astype(int)\n",
    "    model_df_implicit = model_df_implicit.sort_values(by=['debiasing_prompt_id'])\n",
    "    \n",
    "    model_df_explicit['debiasing_prompt_id'] = model_df_explicit['debiasing_prompt_id'].astype(int)\n",
    "    model_df_explicit = model_df_explicit.sort_values(by=['debiasing_prompt_id'])\n",
    "    \n",
    "    model_df_implicit['explicit'] = False\n",
    "    model_df_explicit['explicit'] = True\n",
    "    \n",
    "    model_df = pd.concat([model_df_implicit, model_df_explicit])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:53:29.186870Z",
     "start_time": "2024-10-10T23:53:28.877762Z"
    }
   },
   "id": "49d39b89dc15119f",
   "execution_count": 310
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    debiasing_prompt_id  conversation  female_dominated  male_0_prob  \\\n0                     0         False             False     0.000049   \n1                     0         False              True     0.000003   \n0                     0         False             False     0.847224   \n1                     0         False              True     0.046521   \n2                     1         False             False     0.168421   \n3                     1         False              True     0.047479   \n4                     2         False             False     0.507391   \n5                     2         False              True     0.014372   \n6                     3         False             False     0.333336   \n7                     3         False              True     0.036420   \n8                     4         False             False     0.901424   \n9                     4         False              True     0.066540   \n10                    5         False             False     0.000072   \n11                    5         False              True     0.000026   \n12                    6         False             False     0.000120   \n13                    6         False              True     0.000227   \n\n    female_0_prob  diverse_0_prob    male_0  female_0  diverse_0  male_1_prob  \\\n0        0.000018        0.000017  0.639935  0.197265   0.162795     0.000497   \n1        0.000075        0.000011  0.067075  0.832875   0.100040     0.000086   \n0        0.000017        0.001996  0.949330  0.000070   0.050600     0.658988   \n1        0.886191        0.002237  0.051335  0.945780   0.002890     0.007729   \n2        0.004225        0.010939  0.702165  0.037390   0.260450     0.021094   \n3        0.143553        0.011032  0.240985  0.517625   0.241390     0.004447   \n4        0.000679        0.000648  0.951400  0.004695   0.043885     0.374531   \n5        0.639140        0.000566  0.087720  0.899780   0.012500     0.007123   \n6        0.004590        0.010771  0.792485  0.020075   0.187450     0.376171   \n7        0.281209        0.014362  0.127405  0.670645   0.201955     0.017251   \n8        0.000205        0.000027  0.999745  0.000230   0.000025     0.796573   \n9        0.777044        0.000286  0.104515  0.894955   0.000515     0.042389   \n10       0.000044        0.004780  0.030690  0.013225   0.956105     0.000010   \n11       0.000063        0.005904  0.008355  0.018910   0.972755     0.000004   \n12       0.000110        0.003579  0.035120  0.031065   0.933830     0.000055   \n13       0.000222        0.005278  0.036605  0.038365   0.925040     0.000174   \n\n    ...  male_23  female_23  diverse_23  male_24_prob  female_24_prob  \\\n0   ...  0.80822   0.175630    0.016145      0.000015        0.000003   \n1   ...  0.15274   0.832780    0.014460      0.000003        0.000068   \n0   ...  0.96328   0.001460    0.035255      0.992564        0.000002   \n1   ...  0.06166   0.876085    0.062255      0.049127        0.942991   \n2   ...      NaN        NaN         NaN           NaN             NaN   \n3   ...      NaN        NaN         NaN           NaN             NaN   \n4   ...      NaN        NaN         NaN           NaN             NaN   \n5   ...      NaN        NaN         NaN           NaN             NaN   \n6   ...      NaN        NaN         NaN           NaN             NaN   \n7   ...      NaN        NaN         NaN           NaN             NaN   \n8   ...      NaN        NaN         NaN           NaN             NaN   \n9   ...      NaN        NaN         NaN           NaN             NaN   \n10  ...      NaN        NaN         NaN           NaN             NaN   \n11  ...      NaN        NaN         NaN           NaN             NaN   \n12  ...      NaN        NaN         NaN           NaN             NaN   \n13  ...      NaN        NaN         NaN           NaN             NaN   \n\n    diverse_24_prob   male_24  female_24  diverse_24  explicit  \n0          0.000001  0.844600   0.078855    0.076540     False  \n1          0.000009  0.082110   0.812855    0.105055     False  \n0          0.000340  0.999615   0.000000    0.000385      True  \n1          0.000082  0.050065   0.949845    0.000080      True  \n2               NaN       NaN        NaN         NaN      True  \n3               NaN       NaN        NaN         NaN      True  \n4               NaN       NaN        NaN         NaN      True  \n5               NaN       NaN        NaN         NaN      True  \n6               NaN       NaN        NaN         NaN      True  \n7               NaN       NaN        NaN         NaN      True  \n8               NaN       NaN        NaN         NaN      True  \n9               NaN       NaN        NaN         NaN      True  \n10              NaN       NaN        NaN         NaN      True  \n11              NaN       NaN        NaN         NaN      True  \n12              NaN       NaN        NaN         NaN      True  \n13              NaN       NaN        NaN         NaN      True  \n\n[16 rows x 173 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>debiasing_prompt_id</th>\n      <th>conversation</th>\n      <th>female_dominated</th>\n      <th>male_0_prob</th>\n      <th>female_0_prob</th>\n      <th>diverse_0_prob</th>\n      <th>male_0</th>\n      <th>female_0</th>\n      <th>diverse_0</th>\n      <th>male_1_prob</th>\n      <th>...</th>\n      <th>male_23</th>\n      <th>female_23</th>\n      <th>diverse_23</th>\n      <th>male_24_prob</th>\n      <th>female_24_prob</th>\n      <th>diverse_24_prob</th>\n      <th>male_24</th>\n      <th>female_24</th>\n      <th>diverse_24</th>\n      <th>explicit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000049</td>\n      <td>0.000018</td>\n      <td>0.000017</td>\n      <td>0.639935</td>\n      <td>0.197265</td>\n      <td>0.162795</td>\n      <td>0.000497</td>\n      <td>...</td>\n      <td>0.80822</td>\n      <td>0.175630</td>\n      <td>0.016145</td>\n      <td>0.000015</td>\n      <td>0.000003</td>\n      <td>0.000001</td>\n      <td>0.844600</td>\n      <td>0.078855</td>\n      <td>0.076540</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.000003</td>\n      <td>0.000075</td>\n      <td>0.000011</td>\n      <td>0.067075</td>\n      <td>0.832875</td>\n      <td>0.100040</td>\n      <td>0.000086</td>\n      <td>...</td>\n      <td>0.15274</td>\n      <td>0.832780</td>\n      <td>0.014460</td>\n      <td>0.000003</td>\n      <td>0.000068</td>\n      <td>0.000009</td>\n      <td>0.082110</td>\n      <td>0.812855</td>\n      <td>0.105055</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.847224</td>\n      <td>0.000017</td>\n      <td>0.001996</td>\n      <td>0.949330</td>\n      <td>0.000070</td>\n      <td>0.050600</td>\n      <td>0.658988</td>\n      <td>...</td>\n      <td>0.96328</td>\n      <td>0.001460</td>\n      <td>0.035255</td>\n      <td>0.992564</td>\n      <td>0.000002</td>\n      <td>0.000340</td>\n      <td>0.999615</td>\n      <td>0.000000</td>\n      <td>0.000385</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.046521</td>\n      <td>0.886191</td>\n      <td>0.002237</td>\n      <td>0.051335</td>\n      <td>0.945780</td>\n      <td>0.002890</td>\n      <td>0.007729</td>\n      <td>...</td>\n      <td>0.06166</td>\n      <td>0.876085</td>\n      <td>0.062255</td>\n      <td>0.049127</td>\n      <td>0.942991</td>\n      <td>0.000082</td>\n      <td>0.050065</td>\n      <td>0.949845</td>\n      <td>0.000080</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.168421</td>\n      <td>0.004225</td>\n      <td>0.010939</td>\n      <td>0.702165</td>\n      <td>0.037390</td>\n      <td>0.260450</td>\n      <td>0.021094</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.047479</td>\n      <td>0.143553</td>\n      <td>0.011032</td>\n      <td>0.240985</td>\n      <td>0.517625</td>\n      <td>0.241390</td>\n      <td>0.004447</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.507391</td>\n      <td>0.000679</td>\n      <td>0.000648</td>\n      <td>0.951400</td>\n      <td>0.004695</td>\n      <td>0.043885</td>\n      <td>0.374531</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.014372</td>\n      <td>0.639140</td>\n      <td>0.000566</td>\n      <td>0.087720</td>\n      <td>0.899780</td>\n      <td>0.012500</td>\n      <td>0.007123</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.333336</td>\n      <td>0.004590</td>\n      <td>0.010771</td>\n      <td>0.792485</td>\n      <td>0.020075</td>\n      <td>0.187450</td>\n      <td>0.376171</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.036420</td>\n      <td>0.281209</td>\n      <td>0.014362</td>\n      <td>0.127405</td>\n      <td>0.670645</td>\n      <td>0.201955</td>\n      <td>0.017251</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.901424</td>\n      <td>0.000205</td>\n      <td>0.000027</td>\n      <td>0.999745</td>\n      <td>0.000230</td>\n      <td>0.000025</td>\n      <td>0.796573</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.066540</td>\n      <td>0.777044</td>\n      <td>0.000286</td>\n      <td>0.104515</td>\n      <td>0.894955</td>\n      <td>0.000515</td>\n      <td>0.042389</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>5</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000072</td>\n      <td>0.000044</td>\n      <td>0.004780</td>\n      <td>0.030690</td>\n      <td>0.013225</td>\n      <td>0.956105</td>\n      <td>0.000010</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>5</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.000026</td>\n      <td>0.000063</td>\n      <td>0.005904</td>\n      <td>0.008355</td>\n      <td>0.018910</td>\n      <td>0.972755</td>\n      <td>0.000004</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>6</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000120</td>\n      <td>0.000110</td>\n      <td>0.003579</td>\n      <td>0.035120</td>\n      <td>0.031065</td>\n      <td>0.933830</td>\n      <td>0.000055</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>6</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.000227</td>\n      <td>0.000222</td>\n      <td>0.005278</td>\n      <td>0.036605</td>\n      <td>0.038365</td>\n      <td>0.925040</td>\n      <td>0.000174</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>16 rows  173 columns</p>\n</div>"
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:53:30.627664Z",
     "start_time": "2024-10-10T23:53:30.621327Z"
    }
   },
   "id": "e6db32d5ad00f884",
   "execution_count": 311
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'male'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3790\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3791\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3792\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'male'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[312], line 70\u001B[0m\n\u001B[1;32m     67\u001B[0m row \u001B[38;5;241m=\u001B[39m row_prefix\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m explicit_fd\u001B[38;5;241m.\u001B[39mempty:\n\u001B[0;32m---> 70\u001B[0m     row \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexplicit_fd[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmale\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m% & \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexplicit_fd[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfemale\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m% & \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexplicit_fd[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiverse\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.1f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m% & \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     row \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m & & & \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/core/frame.py:3893\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3891\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3892\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3893\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3895\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/core/indexes/base.py:3798\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3793\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3794\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3795\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3796\u001B[0m     ):\n\u001B[1;32m   3797\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3798\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3799\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3800\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3801\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3802\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3803\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'male'"
     ]
    }
   ],
   "source": [
    " # Update this to your results directory\n",
    "models = list(model_str_map.keys())\n",
    "\n",
    "output_tex_file = os.path.join(results_dir, 'aggregated_results.tex')\n",
    "\n",
    "with open(output_tex_file, 'w') as f_out:\n",
    "    f_out.write(r'\\onecolumn')\n",
    "    for model in models:\n",
    "        model_name = model.replace('-', '')\n",
    "        model_description = model_str_map[model]\n",
    "\n",
    "        f_out.write(r'\\subsection{' + model_description + '}\\n')\n",
    "        # Load the CSV file into a DataFrame\n",
    "        df_implicit = pd.read_csv(os.path.join(results_dir, model, 'implicit.csv'))\n",
    "        df_explicit = pd.read_csv(os.path.join(results_dir, model, 'explicit.csv'))\n",
    "        \n",
    "\n",
    "        # Calculate means for each combination of prompt_id, conversation, and female_dominated\n",
    "        grouped_df_implicit = df_implicit.groupby(['model', 'debiasing_prompt_id', 'conversation', 'female_dominated']).mean(numeric_only=True).reset_index()    \n",
    "        grouped_df_explicit = df_explicit.groupby(['model', 'debiasing_prompt_id', 'conversation', 'female_dominated']).mean(numeric_only=True).reset_index()    \n",
    "\n",
    "        # Drop the 'model' column\n",
    "        model_df_implicit = grouped_df_implicit.drop(columns=['model'])\n",
    "        model_df_explicit = grouped_df_explicit.drop(columns=['model'])\n",
    "        \n",
    "        # Sort by Abs. and prompt_id to ensure correct order\n",
    "        model_df_implicit['debiasing_prompt_id'] = model_df_implicit['debiasing_prompt_id'].astype(int)\n",
    "        model_df_implicit = model_df_implicit.sort_values(by=['debiasing_prompt_id'])\n",
    "        \n",
    "        model_df_explicit['debiasing_prompt_id'] = model_df_explicit['debiasing_prompt_id'].astype(int)\n",
    "        model_df_explicit = model_df_explicit.sort_values(by=['debiasing_prompt_id'])\n",
    "        \n",
    "        model_df_implicit['explicit'] = False\n",
    "        model_df_explicit['explicit'] = True\n",
    "        \n",
    "        model_df = pd.concat([model_df_implicit, model_df_explicit])\n",
    "\n",
    "        latex_table = r'''\n",
    "\\begin{table*}[ht!]\n",
    "\\centering\n",
    "\\small\n",
    "% Reduce text size and slightly the gap between columns\n",
    "% \\setlength{\\tabcolsep}{4.6pt} % Default: 5pt\n",
    "\\caption{Results for ''' + model_str_map[model] + r''' on debiasing prompts.}\n",
    "  \\resizebox{\\textwidth}{!}{  % Alternative method: resize entire table (problem: also resizes line widths)\n",
    "\\begin{tabular}{c c c c c c c c c c c c c c}\n",
    "\\toprule\n",
    "& \\multicolumn{6}{c}{Explicit} & \\multicolumn{6}{c}{Implicit} \\\\\n",
    "\\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n",
    "& \\multicolumn{3}{c}{Female Dominated} & \\multicolumn{3}{c}{Male Dominated} & \\multicolumn{3}{c}{Female Dominated} & \\multicolumn{3}{c}{Male Dominated} \\\\\n",
    "\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n",
    "     ID & M & F & D & M & F & D & M & F & D & M & F & D\\\\\n",
    "    \\midrule\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "        for prompt_id, prompt_group in model_df.groupby('debiasing_prompt_id'):\n",
    "            id_label = 'None' if prompt_id == 0 else str(prompt_id)\n",
    "            row_prefix = f\"        {id_label} & \"\n",
    "\n",
    "            explicit_fd = prompt_group[(prompt_group['explicit'] == True) & (prompt_group['female_dominated'] == True)]\n",
    "            explicit_md = prompt_group[(prompt_group['explicit'] == True) & (prompt_group['female_dominated'] == False)]\n",
    "            implicit_fd = prompt_group[(prompt_group['explicit'] == False) & (prompt_group['female_dominated'] == True)]\n",
    "            implicit_md = prompt_group[(prompt_group['explicit'] == False) & (prompt_group['female_dominated'] == False)]\n",
    "\n",
    "            row = row_prefix\n",
    "\n",
    "            if not explicit_fd.empty:\n",
    "                row += f\"{explicit_fd['male'].values[0]*100:.1f}\\\\% & {explicit_fd['female'].values[0]*100:.1f}\\\\% & {explicit_fd['diverse'].values[0]*100:.1f}\\\\% & \"\n",
    "            else:\n",
    "                row += \" & & & \"\n",
    "\n",
    "            if not explicit_md.empty:\n",
    "                row += f\"{explicit_md['male'].values[0]*100:.1f}\\\\% & {explicit_md['female'].values[0]*100:.1f}\\\\% & {explicit_md['diverse'].values[0]*100:.1f}\\\\% & \"\n",
    "            else:\n",
    "                row += \" & & & \"\n",
    "\n",
    "            if not implicit_fd.empty:\n",
    "                row += f\"{implicit_fd['male'].values[0]*100:.1f}\\\\% & {implicit_fd['female'].values[0]*100:.1f}\\\\% & {implicit_fd['diverse'].values[0]*100:.1f}\\\\% & \"\n",
    "            else:\n",
    "                row += \" & & & \"\n",
    "\n",
    "            if not implicit_md.empty:\n",
    "                row += f\"{implicit_md['male'].values[0]*100:.1f}\\\\% & {implicit_md['female'].values[0]*100:.1f}\\\\% & {implicit_md['diverse'].values[0]*100:.1f}\\\\% \"\n",
    "            else:\n",
    "                row += \" & & \"\n",
    "\n",
    "            row += r\"\\\\\"\n",
    "            latex_table += row + \"\\n\"\n",
    "\n",
    "\n",
    "        latex_table += r'''\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    " } % end \\resizebox\n",
    "\\label{tab:''' + model_name + r'''_debias}\n",
    "\\end{table*}\n",
    "\n",
    "'''\n",
    "\n",
    "        f_out.write(latex_table)\n",
    "        # f_out.write('\\\\clearpage\\n\\n')\n",
    "\n",
    "print(\"Aggregated LaTeX table saved to\", output_tex_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:55:01.171616Z",
     "start_time": "2024-10-10T23:55:01.006480Z"
    }
   },
   "id": "a67c20170ae8ff28",
   "execution_count": 312
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   debiasing_prompt_id  conversation  female_dominated  male_implicit0_prob  \\\n0                    0         False             False             0.000886   \n1                    0         False              True             0.000304   \n0                    0         False             False                  NaN   \n1                    0         False              True                  NaN   \n\n   female_implicit0_prob  diverse_implicit0_prob  male_implicit0  \\\n0               0.000101                0.000016         0.87981   \n1               0.000306                0.000016         0.45324   \n0                    NaN                     NaN             NaN   \n1                    NaN                     NaN             NaN   \n\n   female_implicit0  diverse_implicit0  male_implicit1_prob  ...  \\\n0          0.102205            0.01797              0.00137  ...   \n1          0.525920            0.02085              0.00034  ...   \n0               NaN                NaN                  NaN  ...   \n1               NaN                NaN                  NaN  ...   \n\n   diverse_explicit23_prob  male_explicit23  female_explicit23  \\\n0                      NaN              NaN                NaN   \n1                      NaN              NaN                NaN   \n0                 0.003017         0.727270           0.260405   \n1                 0.003391         0.441395           0.544035   \n\n   diverse_explicit23  male_explicit24_prob  female_explicit24_prob  \\\n0                 NaN                   NaN                     NaN   \n1                 NaN                   NaN                     NaN   \n0            0.012305              0.224377                0.053976   \n1            0.014585              0.087161                0.169080   \n\n   diverse_explicit24_prob  male_explicit24  female_explicit24  \\\n0                      NaN              NaN                NaN   \n1                      NaN              NaN                NaN   \n0                 0.002224         0.799655            0.19227   \n1                 0.002481         0.350665            0.63906   \n\n   diverse_explicit24  \n0                 NaN  \n1                 NaN  \n0            0.008085  \n1            0.010300  \n\n[4 rows x 305 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>debiasing_prompt_id</th>\n      <th>conversation</th>\n      <th>female_dominated</th>\n      <th>male_implicit0_prob</th>\n      <th>female_implicit0_prob</th>\n      <th>diverse_implicit0_prob</th>\n      <th>male_implicit0</th>\n      <th>female_implicit0</th>\n      <th>diverse_implicit0</th>\n      <th>male_implicit1_prob</th>\n      <th>...</th>\n      <th>diverse_explicit23_prob</th>\n      <th>male_explicit23</th>\n      <th>female_explicit23</th>\n      <th>diverse_explicit23</th>\n      <th>male_explicit24_prob</th>\n      <th>female_explicit24_prob</th>\n      <th>diverse_explicit24_prob</th>\n      <th>male_explicit24</th>\n      <th>female_explicit24</th>\n      <th>diverse_explicit24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>0.000886</td>\n      <td>0.000101</td>\n      <td>0.000016</td>\n      <td>0.87981</td>\n      <td>0.102205</td>\n      <td>0.01797</td>\n      <td>0.00137</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>0.000304</td>\n      <td>0.000306</td>\n      <td>0.000016</td>\n      <td>0.45324</td>\n      <td>0.525920</td>\n      <td>0.02085</td>\n      <td>0.00034</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.003017</td>\n      <td>0.727270</td>\n      <td>0.260405</td>\n      <td>0.012305</td>\n      <td>0.224377</td>\n      <td>0.053976</td>\n      <td>0.002224</td>\n      <td>0.799655</td>\n      <td>0.19227</td>\n      <td>0.008085</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.003391</td>\n      <td>0.441395</td>\n      <td>0.544035</td>\n      <td>0.014585</td>\n      <td>0.087161</td>\n      <td>0.169080</td>\n      <td>0.002481</td>\n      <td>0.350665</td>\n      <td>0.63906</td>\n      <td>0.010300</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows  305 columns</p>\n</div>"
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T05:16:57.027496Z",
     "start_time": "2024-10-10T05:16:57.014902Z"
    }
   },
   "id": "8f08f80f208b3d78",
   "execution_count": 286
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/results_debias/llama3-8b/non_gq.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[313], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model, file_name \u001B[38;5;129;01min\u001B[39;00m product(models, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnon_gq.csv\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# Load the CSV file into a DataFrame\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     df_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(results_dir, model, file_name)\n\u001B[0;32m---> 13\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     df_0\u001B[38;5;241m.\u001B[39mappend(df[df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdebiasing_prompt_id\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     16\u001B[0m df_0 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(df_0)\n",
      "File \u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    936\u001B[0m     dialect,\n\u001B[1;32m    937\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    945\u001B[0m )\n\u001B[1;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/anaconda3/envs/gender/lib/python3.9/site-packages/pandas/io/common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    866\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/results_debias/llama3-8b/non_gq.csv'"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Collect all prompt_id = 0\n",
    "df_0 = []\n",
    "\n",
    "\n",
    "\n",
    "for model, file_name in product(models, ['non_gq.csv']):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df_path = os.path.join(results_dir, model, file_name)\n",
    "    df = pd.read_csv(df_path)\n",
    "    df_0.append(df[df['debiasing_prompt_id'] == 0])\n",
    "\n",
    "df_0 = pd.concat(df_0)\n",
    "\n",
    "# Calculate means for each combination of model, conversation, and female_dominated\n",
    "grouped_df_0 = df_0.groupby(['model', 'conversation', 'female_dominated']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Drop the 'debiasing_prompt_id' column as it is always 0 in this filtered DataFrame\n",
    "grouped_df_0 = grouped_df_0.drop(columns=['debiasing_prompt_id'])\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = r'''\n",
    "\\begin{table*}[t]\n",
    "\\centering\n",
    "\\small\n",
    "    % Reduce text size and slightly the gap between columns\n",
    "    \\setlength{\\tabcolsep}{4.6pt} % Default: 5pt\n",
    "    % \\resizebox{\\textwidth}{!}{  % Alternative method: resize entire table (problem: also resizes line widths)\n",
    "    \\begin{tabular}{l c c c c c c}\n",
    "    \\toprule\n",
    "    & \\multicolumn{3}{c}{Female Dominated} & \\multicolumn{3}{c}{Male Dominated} \\\\\n",
    "    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n",
    "    Model & M & F & D & M & F & D \\\\\n",
    "    \\midrule\n",
    "'''\n",
    "\n",
    "for model in models:\n",
    "    model_name = model_str_map[model]\n",
    "    no_dialogue_fd = grouped_df_0[(grouped_df_0['model'] == model) & (grouped_df_0['conversation'] == False) & (grouped_df_0['female_dominated'] == True)]\n",
    "    no_dialogue_md = grouped_df_0[(grouped_df_0['model'] == model) & (grouped_df_0['conversation'] == False) & (grouped_df_0['female_dominated'] == False)]\n",
    "    \n",
    "    row_prefix = f\"        {model_name} & \"\n",
    "    row = row_prefix\n",
    "    \n",
    "    if not no_dialogue_fd.empty:\n",
    "        row += f\"{no_dialogue_fd['male'].values[0]*100:.1f}\\\\% & {no_dialogue_fd['female'].values[0]*100:.1f}\\\\% & {no_dialogue_fd['diverse'].values[0]*100:.1f}\\\\% & \"\n",
    "    else:\n",
    "        row += \" & & & \"\n",
    "    \n",
    "    if not no_dialogue_md.empty:\n",
    "        row += f\"{no_dialogue_md['male'].values[0]*100:.1f}\\\\% & {no_dialogue_md['female'].values[0]*100:.1f}\\\\% & {no_dialogue_md['diverse'].values[0]*100:.1f}\\\\% \"\n",
    "    else:\n",
    "        row += \" & & \"\n",
    "    \n",
    "    row += r\"\\\\\"\n",
    "    # do not add midrule if model is the last in models\n",
    "    if \"instruct\" in model and model != models[-1]:\n",
    "        latex_table += row + \"\\n        \\\\midrule\\n\"\n",
    "    else:\n",
    "        latex_table += row + \"\\n        \\n\"\n",
    "\n",
    "latex_table += r'''\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "% } % end \\resizebox\n",
    "\\caption{Results for all models on implicit bias (average over task prompts 2-4).}\n",
    "\\label{tab:implicit_default}\n",
    "\\end{table*}\n",
    "'''\n",
    "\n",
    "# Save the LaTeX table to a file\n",
    "output_tex_file = os.path.join(results_dir, 'implicit_default.tex')\n",
    "with open(output_tex_file, 'w') as f_out:\n",
    "    f_out.write(latex_table)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Save the LaTeX table to a file\n",
    "# output_tex_file = os.path.join(results_dir, 'default.tex')\n",
    "# with open(output_tex_file, 'w') as f_out:\n",
    "#     f_out.write(latex_table_explicit)\n",
    "#     f_out.write('\\n\\n')\n",
    "#     f_out.write(latex_table)\n",
    "\n",
    "print(\"Aggregated LaTeX table saved to\", output_tex_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T23:55:06.135220Z",
     "start_time": "2024-10-10T23:55:06.085714Z"
    }
   },
   "id": "2cb37de0ed62e51a",
   "execution_count": 313
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf7d61746c7f3b45"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
