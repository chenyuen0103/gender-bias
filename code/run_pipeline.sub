executable = /home/yuenchen/anaconda3/envs/llama/bin/python
arguments = /home/yuenchen/gender-bias/code/pipeline_opensrc.py --model $(model)
error = /home/yuenchen/gender-bias/pipeline_copy_mistral.err
output = /home/yuenchen/gender-bias/pipeline_copy_mistral.out
log = /home/yuenchen/gender-bias/pipeline_copy_mistral.log
request_memory = 16G
request_disk = 20G
request_cpus = 1
request_gpus = 1
requirements = TARGET.CUDAGlobalMemoryMb  > 16000
queue model in llama3-8b-instruct mistral-7b mistral-7b-instruct llama2-7b llama2-7b-chat
