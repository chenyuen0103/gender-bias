executable = /home/yuenchen/gender-bias/code/run_pipeline.sh
arguments = $(type) $(model)
error = /home/yuenchen/gender-bias/pipeline_$(type)$(model).err
output = /home/yuenchen/gender-bias/pipeline_$(type)$(model).out
log = /home/yuenchen/gender-bias/pipeline_$(type)$(model).log
request_memory = 16G
request_disk = 20G
request_cpus = 1
request_gpus = 1
requirements = TARGET.CUDAGlobalMemoryMb  > 16000
queue type, model from (
     llama3-8b
     llama3-8b-instruct
     mistral-7b
     mistral-7b-instruct
     llama2-7b
     llama2-7b-chat
    conversation_ llama3-8b
    conversation_ llama3-8b-instruct
    conversation_ mistral-7b
    conversation_ mistral-7b-instruct
    conversation_ llama2-7b
    conversation_ llama2-7b-chat
)
