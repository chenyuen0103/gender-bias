,"GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_male_met-met","GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_male_friend","GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_male_talk-met","GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_female_met-met","GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_female_friend","GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_female_talk-met","GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_diverse_met-met","GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_diverse_friend","GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)_diverse_talk-met"
0,0.29,0.2236,0.5018,0.343,0.1624,0.2419,0.367,0.614,0.2564
1,0.3231,0.2268,0.7507,0.3858,0.3344,0.1361,0.2911,0.4388,0.1132
2,0.3293,0.3104,0.7,0.4242,0.1917,0.2011,0.2465,0.4979,0.0989
3,0.3121,0.2218,0.4994,0.3526,0.2874,0.2044,0.3353,0.4907,0.2962
4,0.3276,0.1406,0.4782,0.3782,0.1578,0.1863,0.2942,0.7017,0.3355
5,0.3177,0.378,0.5824,0.3895,0.2044,0.2471,0.2928,0.4177,0.1706
6,0.1669,0.3428,0.7443,0.3125,0.2026,0.0985,0.5206,0.4546,0.1572
7,0.2985,0.2609,0.6251,0.3524,0.2402,0.2709,0.3491,0.499,0.1041
8,0.2776,0.5043,0.5415,0.3135,0.3428,0.2493,0.4089,0.153,0.2092
9,0.2513,0.3818,0.546,0.2902,0.473,0.2154,0.4585,0.1452,0.2386
10,0.2774,0.3081,0.5845,0.3474,0.3247,0.2203,0.3752,0.3673,0.1953
11,0.1718,0.2761,0.5407,0.2847,0.2016,0.182,0.5435,0.5223,0.2772
12,0.3515,0.1981,0.5612,0.3532,0.1292,0.2028,0.2953,0.6727,0.236
13,0.2895,0.4128,0.5073,0.3757,0.2301,0.3215,0.3348,0.3572,0.1712
14,0.1963,0.312,0.5066,0.1267,0.449,0.2658,0.6771,0.239,0.2275
15,0.2996,0.2386,0.5,0.3737,0.2695,0.1441,0.3268,0.4919,0.3559
16,0.325,0.3156,0.5128,0.3747,0.3156,0.302,0.3003,0.3687,0.1852
17,0.4251,0.1878,0.4429,0.334,0.3892,0.2651,0.2409,0.423,0.292
18,0.3283,0.318,0.5658,0.3402,0.2798,0.1712,0.3315,0.4022,0.263
19,0.3192,0.3191,0.699,0.378,0.4104,0.1285,0.3028,0.2705,0.1725
20,0.1932,0.31,0.4517,0.1555,0.5521,0.2927,0.6513,0.1379,0.2556
21,0.3263,0.3834,0.4537,0.2171,0.4817,0.243,0.4565,0.1349,0.3033
22,0.388,0.2495,0.3586,0.3781,0.3549,0.1972,0.2338,0.3956,0.4442
23,0.3555,0.2364,0.4152,0.3475,0.2652,0.3147,0.297,0.4983,0.2701
24,0.3997,0.4188,0.5055,0.2313,0.3915,0.3104,0.369,0.1897,0.1841
25,0.3708,0.1374,0.4035,0.4009,0.1967,0.3949,0.2283,0.6659,0.2016
26,0.352,0.2657,0.6147,0.3835,0.2785,0.2843,0.2645,0.4558,0.101
27,0.1262,0.0445,0.3751,0.1246,0.0528,0.2452,0.7491,0.9027,0.3796
28,0.3309,0.3004,0.5576,0.3952,0.3231,0.2925,0.2739,0.3766,0.1499
29,0.323,0.2965,0.4811,0.2807,0.4153,0.2923,0.3963,0.2882,0.2266
30,0.3593,0.29,0.598,0.3247,0.3166,0.2772,0.316,0.3934,0.1247
31,0.505,0.2801,0.4415,0.3528,0.3225,0.1682,0.1422,0.3974,0.3903
32,0.3943,0.351,0.5129,0.2961,0.3792,0.2901,0.3096,0.2697,0.197
33,0.4039,0.3794,0.3539,0.3773,0.4158,0.3465,0.2188,0.2048,0.2996
34,0.3004,0.2151,0.4652,0.231,0.3257,0.2737,0.4686,0.4592,0.2611
35,0.4029,0.3311,0.5505,0.3311,0.4371,0.2116,0.2661,0.2318,0.2379
36,0.3694,0.2884,0.5746,0.443,0.495,0.2317,0.1876,0.2166,0.1937
37,0.391,0.2913,0.4766,0.3725,0.3446,0.2496,0.2365,0.364,0.2737
38,0.4052,0.3634,0.4407,0.3773,0.3187,0.2975,0.2174,0.3179,0.2618
39,0.3843,0.3678,0.4112,0.3342,0.4686,0.3164,0.2815,0.1636,0.2724
